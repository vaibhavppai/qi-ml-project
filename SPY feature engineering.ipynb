{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T14:15:04.747048Z",
     "start_time": "2021-05-26T14:14:53.340719Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-01T13:47:25.114408Z",
     "iopub.status.busy": "2022-12-01T13:47:25.112415Z",
     "iopub.status.idle": "2022-12-01T13:47:31.164596Z",
     "shell.execute_reply": "2022-12-01T13:47:31.164596Z",
     "shell.execute_reply.started": "2022-12-01T13:47:25.112415Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2022-12-01T19:17:25.211184+05:30\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.8\n",
      "IPython version      : 8.4.0\n",
      "\n",
      "Compiler    : MSC v.1916 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 165 Stepping 2, GenuineIntel\n",
      "CPU cores   : 12\n",
      "Architecture: 64bit\n",
      "\n",
      "\n",
      " 19b MLT 04 How to Develop Trading Strategies using Machine Learning\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.8\n",
      "IPython version      : 8.4.0\n",
      "\n",
      "numpy     : 1.21.5\n",
      "pandas    : 1.3.2\n",
      "scipy     : 1.6.2\n",
      "sklearn   : 1.1.3\n",
      "mlfinlab  : not installed\n",
      "seaborn   : 0.11.1\n",
      "matplotlib: 3.5.2\n",
      "\n",
      "Compiler    : MSC v.1916 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 165 Stepping 2, GenuineIntel\n",
      "CPU cores   : 12\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import standard libs\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import display\n",
    "from IPython.core.debugger import set_trace as debug\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def get_relative_project_dir(project_repo_name=None, partial=True):\n",
    "    \"\"\"helper fn to get local project directory\"\"\"\n",
    "    current_working_directory = Path.cwd()\n",
    "    cwd_parts = current_working_directory.parts\n",
    "    if partial:\n",
    "        while project_repo_name not in cwd_parts[-1]:\n",
    "            current_working_directory = current_working_directory.parent\n",
    "            cwd_parts = current_working_directory.parts\n",
    "    else:\n",
    "        while cwd_parts[-1] != project_repo_name:\n",
    "            current_working_directory = current_working_directory.parent\n",
    "            cwd_parts = current_working_directory.parts\n",
    "    return current_working_directory\n",
    "\n",
    "\n",
    "# import python scientific stack\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import numba as nb\n",
    "\n",
    "# import ffn\n",
    "import yfinance as yf\n",
    "import bottleneck as bk\n",
    "import mlxtend as mlx\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "import numpy_ext as npx\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "import shap\n",
    "\n",
    "# import visual tools\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# import util libs\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# THESE ARE VARIABLES FOR EASILY ACCESSING DIFFERENT\n",
    "# DIRECTORIES FOR ACCESSING AND SAVING DATA AND IMAGES\n",
    "# IF NECESSARY. CHANGE THEM TO MATCH YOUR DIRECTORY\n",
    "# STRUCTURE.\n",
    "# ---------------------------------------------------\n",
    "\n",
    "REPO_NAME = \"19b MLT 04 How to Develop Trading Strategies using Machine Learning\" # CHANGE TO MATCH YOUR REPO NAME\n",
    "print(\"\\n\", REPO_NAME)\n",
    "project_dir = get_relative_project_dir(REPO_NAME)\n",
    "data_dir = project_dir / \"data\"\n",
    "external = data_dir / \"external\"\n",
    "processed = data_dir / \"processed\"\n",
    "viz = project_dir / \"viz\"\n",
    "\n",
    "\n",
    "def cprint(df: pd.DataFrame, nrows: int = None):\n",
    "    \"\"\"\n",
    "    Custom dataframe print function\n",
    "    \"\"\"\n",
    "    if not isinstance(df, (pd.DataFrame,)):\n",
    "        try:\n",
    "            df = df.to_frame()\n",
    "        except:\n",
    "            raise ValueError(\"object cannot be coerced to df\")\n",
    "\n",
    "    if not nrows:\n",
    "        nrows = 5\n",
    "    print(\"*\" * 79)\n",
    "    print(\"dataframe information\")\n",
    "    print(\"-\" * 79)\n",
    "    print(f\"HEAD num rows: {nrows}\")\n",
    "    print(df.head(nrows))\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"TAIL num rows: {nrows}\")\n",
    "    print(df.tail(nrows))\n",
    "    print(\"-\" * 50)\n",
    "    print(df.info())\n",
    "    print(\"*\" * 79)\n",
    "    print()\n",
    "    return\n",
    "\n",
    "\n",
    "print()\n",
    "%watermark -v -m -p numpy,pandas,scipy,sklearn,mlfinlab,seaborn,matplotlib -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T14:15:04.953120Z",
     "start_time": "2021-05-26T14:15:04.750047Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-01T13:47:31.166579Z",
     "iopub.status.busy": "2022-12-01T13:47:31.166579Z",
     "iopub.status.idle": "2022-12-01T13:47:31.368724Z",
     "shell.execute_reply": "2022-12-01T13:47:31.368724Z",
     "shell.execute_reply.started": "2022-12-01T13:47:31.166579Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\nsns_params = {\\n    \\\"xtick.major.size\\\": 2,\\n    \\\"ytick.major.size\\\": 2,\\n    \\\"font.size\\\": 12,\\n    \\\"font.weight\\\": \\\"medium\\\",\\n    \\\"figure.figsize\\\": (10, 7),\\n    # \\\"font.family\\\": \\\"Ubuntu Mono\\\",\\n}\\nsns.set_theme(\\n    context=\\\"talk\\\",\\n    style=\\\"white\\\",\\n    # palette=sns.color_palette(\\\"magma\\\"),\\n    rc=sns_params,\\n)\\nsavefig_kwds = dict(dpi=90, bbox_inches=\\\"tight\\\", frameon=True, format=\\\"png\\\")\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\nsns_params = {\\n    \\\"xtick.major.size\\\": 2,\\n    \\\"ytick.major.size\\\": 2,\\n    \\\"font.size\\\": 12,\\n    \\\"font.weight\\\": \\\"medium\\\",\\n    \\\"figure.figsize\\\": (10, 7),\\n    # \\\"font.family\\\": \\\"Ubuntu Mono\\\",\\n}\\nsns.set_theme(\\n    context=\\\"talk\\\",\\n    style=\\\"white\\\",\\n    # palette=sns.color_palette(\\\"magma\\\"),\\n    rc=sns_params,\\n)\\nsavefig_kwds = dict(dpi=90, bbox_inches=\\\"tight\\\", frameon=True, format=\\\"png\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "sns_params = {\n",
    "    \"xtick.major.size\": 2,\n",
    "    \"ytick.major.size\": 2,\n",
    "    \"font.size\": 12,\n",
    "    \"font.weight\": \"medium\",\n",
    "    \"figure.figsize\": (10, 7),\n",
    "    # \"font.family\": \"Ubuntu Mono\",\n",
    "}\n",
    "sns.set_theme(\n",
    "    context=\"talk\",\n",
    "    style=\"white\",\n",
    "    # palette=sns.color_palette(\"magma\"),\n",
    "    rc=sns_params,\n",
    ")\n",
    "savefig_kwds = dict(dpi=90, bbox_inches=\"tight\", frameon=True, format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T23:51:00.624933Z",
     "start_time": "2021-03-29T23:50:52.574296Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-01T13:47:31.369746Z",
     "iopub.status.busy": "2022-12-01T13:47:31.369746Z",
     "iopub.status.idle": "2022-12-01T13:47:35.719967Z",
     "shell.execute_reply": "2022-12-01T13:47:35.719967Z",
     "shell.execute_reply.started": "2022-12-01T13:47:31.369746Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "dataframe information\n",
      "-------------------------------------------------------------------------------\n",
      "HEAD num rows: 5\n",
      "                       open    high     low   close      up    down   volume\n",
      "datetime                                                                    \n",
      "2004-05-14 06:31:00  109.96  110.00  109.90  109.99  133400  601500   734900\n",
      "2004-05-14 06:32:00  110.00  110.18  109.98  110.09  118300   52500   170800\n",
      "2004-05-14 06:33:00  110.09  110.30  110.05  110.29   94100   98900   193000\n",
      "2004-05-14 06:34:00  110.28  110.44  110.22  110.39  170400  876600  1047000\n",
      "2004-05-14 06:35:00  110.41  110.41  110.22  110.26  150100  359900   510000\n",
      "-------------------------\n",
      "TAIL num rows: 5\n",
      "                       open    high     low   close      up     down   volume\n",
      "datetime                                                                     \n",
      "2019-05-14 12:56:00  283.77  283.86  283.69  283.74  313916   322548   636464\n",
      "2019-05-14 12:57:00  283.74  283.74  283.42  283.44  154415   244134   398549\n",
      "2019-05-14 12:58:00  283.43  283.60  283.40  283.57  240404   316421   556825\n",
      "2019-05-14 12:59:00  283.58  283.59  283.42  283.42  308214   493246   801460\n",
      "2019-05-14 13:00:00  283.42  283.49  283.17  283.32  933738  1283851  2217589\n",
      "--------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1468155 entries, 2004-05-14 06:31:00 to 2019-05-14 13:00:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count    Dtype  \n",
      "---  ------  --------------    -----  \n",
      " 0   open    1468155 non-null  float64\n",
      " 1   high    1468155 non-null  float64\n",
      " 2   low     1468155 non-null  float64\n",
      " 3   close   1468155 non-null  float64\n",
      " 4   up      1468155 non-null  int64  \n",
      " 5   down    1468155 non-null  int64  \n",
      " 6   volume  1468155 non-null  int64  \n",
      "dtypes: float64(4), int64(3)\n",
      "memory usage: 89.6 MB\n",
      "None\n",
      "*******************************************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"def read_tradestation_data(fn: Path) -> pd.DataFrame:\\n    df = pd.read_csv(fn).rename(str.lower, axis=\\\"columns\\\")\\n    df[\\\"datetime\\\"] = pd.to_datetime(\\n        df[\\\"date\\\"] + \\\" \\\" + df[\\\"time\\\"], infer_datetime_format=True\\n    )\\n    df[\\\"volume\\\"] = df[\\\"up\\\"] + df[\\\"down\\\"]\\n    df.drop([\\\"date\\\", \\\"time\\\"], axis=1, inplace=True)\\n    df.set_index(\\\"datetime\\\", inplace=True)\\n    return df\\n\\n\\nspy = read_tradestation_data(external / \\\"tradestation\\\" / \\\"SPY.txt\\\")\\ncprint(spy)\";\n",
       "                var nbb_formatted_code = \"def read_tradestation_data(fn: Path) -> pd.DataFrame:\\n    df = pd.read_csv(fn).rename(str.lower, axis=\\\"columns\\\")\\n    df[\\\"datetime\\\"] = pd.to_datetime(\\n        df[\\\"date\\\"] + \\\" \\\" + df[\\\"time\\\"], infer_datetime_format=True\\n    )\\n    df[\\\"volume\\\"] = df[\\\"up\\\"] + df[\\\"down\\\"]\\n    df.drop([\\\"date\\\", \\\"time\\\"], axis=1, inplace=True)\\n    df.set_index(\\\"datetime\\\", inplace=True)\\n    return df\\n\\n\\nspy = read_tradestation_data(external / \\\"tradestation\\\" / \\\"SPY.txt\\\")\\ncprint(spy)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_tradestation_data(fn: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(fn).rename(str.lower, axis=\"columns\")\n",
    "    df[\"datetime\"] = pd.to_datetime(\n",
    "        df[\"date\"] + \" \" + df[\"time\"], infer_datetime_format=True\n",
    "    )\n",
    "    df[\"volume\"] = df[\"up\"] + df[\"down\"]\n",
    "    df.drop([\"date\", \"time\"], axis=1, inplace=True)\n",
    "    df.set_index(\"datetime\", inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "spy = read_tradestation_data(external / \"tradestation\" / \"SPY.txt\")\n",
    "cprint(spy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T23:51:02.909188Z",
     "start_time": "2021-03-29T23:51:00.629921Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-01T13:47:35.721961Z",
     "iopub.status.busy": "2022-12-01T13:47:35.721961Z",
     "iopub.status.idle": "2022-12-01T13:47:36.757875Z",
     "shell.execute_reply": "2022-12-01T13:47:36.757875Z",
     "shell.execute_reply.started": "2022-12-01T13:47:35.721961Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"def time_consolidator(df, period):\\n    aggregate = {\\n        \\\"open\\\": \\\"first\\\",\\n        \\\"high\\\": \\\"max\\\",\\n        \\\"low\\\": \\\"min\\\",\\n        \\\"close\\\": \\\"last\\\",\\n        \\\"up\\\": \\\"sum\\\",\\n        \\\"down\\\": \\\"sum\\\",\\n        \\\"volume\\\": \\\"sum\\\",\\n    }\\n    return df.resample(f\\\"{period}Min\\\").agg(aggregate).dropna()\\n\\n\\ndata = dict()\\ndf = spy.copy()\\nbars_5m = time_consolidator(df, 5)\\nbars_30m = time_consolidator(df, 30)\\nbars_1H = time_consolidator(df, 60)\\nbars_1D = time_consolidator(df, 60 * 24)\\nbars_1W = time_consolidator(df, 60 * 24 * 7)\\n\\ndata = {\\n    \\\"5m\\\": bars_5m,\\n    \\\"30m\\\": bars_30m,\\n    \\\"1H\\\": bars_1H,\\n    \\\"1D\\\": bars_1D,\\n    \\\"1W\\\": bars_1W,\\n}\";\n",
       "                var nbb_formatted_code = \"def time_consolidator(df, period):\\n    aggregate = {\\n        \\\"open\\\": \\\"first\\\",\\n        \\\"high\\\": \\\"max\\\",\\n        \\\"low\\\": \\\"min\\\",\\n        \\\"close\\\": \\\"last\\\",\\n        \\\"up\\\": \\\"sum\\\",\\n        \\\"down\\\": \\\"sum\\\",\\n        \\\"volume\\\": \\\"sum\\\",\\n    }\\n    return df.resample(f\\\"{period}Min\\\").agg(aggregate).dropna()\\n\\n\\ndata = dict()\\ndf = spy.copy()\\nbars_5m = time_consolidator(df, 5)\\nbars_30m = time_consolidator(df, 30)\\nbars_1H = time_consolidator(df, 60)\\nbars_1D = time_consolidator(df, 60 * 24)\\nbars_1W = time_consolidator(df, 60 * 24 * 7)\\n\\ndata = {\\n    \\\"5m\\\": bars_5m,\\n    \\\"30m\\\": bars_30m,\\n    \\\"1H\\\": bars_1H,\\n    \\\"1D\\\": bars_1D,\\n    \\\"1W\\\": bars_1W,\\n}\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def time_consolidator(df, period):\n",
    "    aggregate = {\n",
    "        \"open\": \"first\",\n",
    "        \"high\": \"max\",\n",
    "        \"low\": \"min\",\n",
    "        \"close\": \"last\",\n",
    "        \"up\": \"sum\",\n",
    "        \"down\": \"sum\",\n",
    "        \"volume\": \"sum\",\n",
    "    }\n",
    "    return df.resample(f\"{period}Min\").agg(aggregate).dropna()\n",
    "\n",
    "\n",
    "data = dict()\n",
    "df = spy.copy()\n",
    "bars_5m = time_consolidator(df, 5)\n",
    "bars_30m = time_consolidator(df, 30)\n",
    "bars_1H = time_consolidator(df, 60)\n",
    "bars_1D = time_consolidator(df, 60 * 24)\n",
    "bars_1W = time_consolidator(df, 60 * 24 * 7)\n",
    "\n",
    "data = {\n",
    "    \"5m\": bars_5m,\n",
    "    \"30m\": bars_30m,\n",
    "    \"1H\": bars_1H,\n",
    "    \"1D\": bars_1D,\n",
    "    \"1W\": bars_1W,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Functions and Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T23:51:03.745614Z",
     "start_time": "2021-03-29T23:51:02.913178Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-01T13:47:36.759896Z",
     "iopub.status.busy": "2022-12-01T13:47:36.758887Z",
     "iopub.status.idle": "2022-12-01T13:47:37.118925Z",
     "shell.execute_reply": "2022-12-01T13:47:37.117959Z",
     "shell.execute_reply.started": "2022-12-01T13:47:36.759896Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"#################\\n\\n\\ndef add_log_returns(df: pd.DataFrame, column: str) -> pd.DataFrame:\\n    df[f\\\"log_{column}_return\\\"] = np.log(df[column]).diff()\\n    return df\\n\\n\\n#################\\ndef internal_bar_strength(df: pd.DataFrame) -> float:\\n    return (df.close - df.low) / (df.high - df.low)\\n\\n\\ndef add_internal_bar_strength(df: pd.DataFrame) -> pd.DataFrame:\\n    df[\\\"ibs\\\"] = internal_bar_strength(df)\\n    return df\\n\\n\\n#################\\n# @nb.jit\\ndef aqr_momentum(array: np.ndarray) -> float:\\n    \\\"\\\"\\\"\\n    Input:  Price time series.\\n    Output: Annualized exponential regression slope, \\n            multiplied by the R2\\n    \\\"\\\"\\\"    \\n    returns = np.diff(np.log(array))  # .diff()\\n    x = np.arange(len(returns))\\n    slope, _, rvalue, _, _ = stats.linregress(x, returns)\\n    return ((1 + slope) ** 252) * (rvalue ** 2)  # annualize slope and multiply by R^2\\n\\n\\n@nb.njit\\ndef aqr_momo_numba(array: np.ndarray) -> float:\\n    y = np.diff(np.log(array))\\n    x = np.arange(y.shape[0])\\n    A = np.column_stack((x, np.ones(x.shape[0])))\\n    model, resid = np.linalg.lstsq(A, y)[:2]\\n    r2 = 1 - resid / (y.size * y.var())\\n    return (((1 + model[0]) ** 252) * r2)[0]\\n\\n\\ndef add_aqr_momentum(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"aqr_momo_{column}_{window}\\\"] = npx.rolling_apply(\\n        aqr_momentum, window, df[column].values, n_jobs=10\\n    )\\n    return df\\n\\n\\ndef add_aqr_momentum_numba(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"aqr_momo_{column}_{window}\\\"] = npx.rolling_apply(\\n        aqr_momo_numba, window, df[column].values, n_jobs=10\\n    )\\n    return df\\n\\n\\n#################\\n\\n\\ndef get_slope(array: np.ndarray) -> float:\\n    returns = np.diff(np.log(array))\\n    x = np.arange(len(returns))\\n    slope, _, rvalue, _, _ = stats.linregress(x, returns)\\n    return slope\\n\\n\\n@nb.njit\\ndef get_slope_numba(array: np.ndarray) -> float:\\n    y = np.diff(np.log(array))\\n    # y = y[~np.isnan(y)]\\n    x = np.arange(y.shape[0])\\n    A = np.column_stack((x, np.ones(x.shape[0])))\\n    model, resid = np.linalg.lstsq(A, y)[:2]\\n    return model[0]\\n\\n\\ndef add_slope_column(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"slope_{column}_{window}\\\"] = npx.rolling_apply(\\n        get_slope, window, df[column].values, n_jobs=10\\n    )\\n    return df\\n\\n\\ndef add_slope_column_numba(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"slope_{column}_{window}\\\"] = npx.rolling_apply(\\n        get_slope_numba, window, df[column].values, n_jobs=1\\n    )\\n    return df\\n\\n\\n#################\\ndef add_average_price(df: pd.DataFrame) -> pd.DataFrame:\\n    df[\\\"average_price\\\"] = (df.high + df.low + df.close + df.open) / 4\\n    return df\\n\\n\\n#################\\ndef add_rolling_min(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    array = npx.rolling_apply(np.min, window, df[column].values, n_jobs=10)\\n    df[f\\\"rmin_{column}_{window}\\\"] = array\\n    return df\\n\\n\\ndef add_rolling_max(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    array = npx.rolling_apply(np.max, window, df[column].values, n_jobs=10)\\n    df[f\\\"rmax_{column}_{window}\\\"] = array\\n    return df\\n\\n\\n#################\\n\\n# for some reason njit is generating zerodivision errors whereas numpy is not\\n@nb.njit\\ndef numba_vwap(\\n    avg: np.ndarray, v: np.ndarray, idx: np.ndarray, len_df: int, window: int\\n) -> np.ndarray:\\n    n = np.shape(np.arange(len_df - window))[0]\\n    A = np.empty((n, 2))\\n    for i in np.arange(len_df - window):\\n        tmp_avg = avg[i : i + window]\\n        tmp_v = v[i : i + window]\\n        aa = np.sum(tmp_v * tmp_avg) / np.sum(tmp_v)\\n        jj = idx[i + window]\\n        A[i, 0] = jj\\n        A[i, 1] = aa\\n    return A\\n\\n\\ndef numpy_vwap(\\n    avg: np.ndarray, v: np.ndarray, idx: np.ndarray, len_df: int, window: int\\n) -> np.ndarray:\\n    n = np.shape(np.arange(len_df - window))[0]\\n    A = np.empty((n, 2))\\n    for i in tqdm(np.arange(len_df - window)):\\n        tmp_avg = avg[i : i + window]\\n        tmp_v = v[i : i + window]\\n        aa = np.sum(tmp_v * tmp_avg) / np.sum(tmp_v)\\n        jj = idx[i + window]\\n        A[i, 0] = jj\\n        A[i, 1] = aa\\n    return A\\n\\n\\ndef add_rolling_vwap(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    v = df.volume.values\\n    avg = df[column].values\\n    idx = df.index.asi8\\n    # A = numba_vwap(avg, v, idx, len(df), window)\\n    A = numpy_vwap(avg, v, idx, len(df), window)\\n    outdf = (\\n        pd.DataFrame(A, columns=[\\\"index\\\", f\\\"rvwap_{window}\\\"])\\n        .assign(datetime=lambda df: pd.to_datetime(df[\\\"index\\\"], unit=\\\"ns\\\"))\\n        .drop(\\\"index\\\", axis=1)\\n        .set_index(\\\"datetime\\\")\\n    )\\n    df = df.join(outdf, how=\\\"left\\\")\\n    return df\\n\\n\\n#################\\ndef add_rolling_bands(\\n    df: pd.DataFrame, column: str, dist: int, window: int\\n) -> pd.DataFrame:\\n    upper = df[column] + dist * df[column].rolling(window).std()\\n    lower = df[column] - dist * df[column].rolling(window).std()\\n\\n    df[f\\\"upper_band_{column}\\\"] = upper\\n    df[f\\\"lower_band_{column}\\\"] = lower\\n    return df\\n\\n\\n#################\\ndef add_acceleration(\\n    df: pd.DataFrame, column: str = \\\"close\\\", window: int = 10\\n) -> pd.DataFrame:\\n    return_diff = df[column].pct_change().diff()\\n    df[f\\\"racc_{column}_{window}\\\"] = return_diff.rolling(\\n        window\\n    ).std()  # standard deviation of second deriv aka acceleration\\n    return df\\n\\n\\ndef roll_rank_bk(array):\\n    rank = array.size + 1 - bk.rankdata(array)[-1]\\n    A = array.shape[0]\\n    p = rank / A\\n    return p\\n\\n\\n#################\\ndef add_volatility(\\n    df: pd.DataFrame, column: str = \\\"close\\\", window: int = 10\\n) -> pd.DataFrame:\\n    returns = df[column].pct_change()\\n    df[f\\\"rvol_{column}_{window}\\\"] = returns.rolling(window).std()\\n    return df\\n\\n\\ndef relative_strength_index(df: pd.DataFrame, n: int) -> pd.Series:\\n    \\\"\\\"\\\"\\n    Calculate Relative Strength Index(RSI) for given data.\\n    https://github.com/Crypto-toolbox/pandas-technical-indicators/blob/master/technical_indicators.py\\n\\n    :param df: pandas.DataFrame\\n    :param n:\\n    :return: pandas.DataFrame\\n    \\\"\\\"\\\"\\n    i = 0\\n    UpI = [0]\\n    DoI = [0]\\n    while i + 1 <= df.index[-1]:\\n        UpMove = df.loc[i + 1, \\\"high\\\"] - df.loc[i, \\\"high\\\"]\\n        DoMove = df.loc[i, \\\"low\\\"] - df.loc[i + 1, \\\"low\\\"]\\n        if UpMove > DoMove and UpMove > 0:\\n            UpD = UpMove\\n        else:\\n            UpD = 0\\n        UpI.append(UpD)\\n        if DoMove > UpMove and DoMove > 0:\\n            DoD = DoMove\\n        else:\\n            DoD = 0\\n        DoI.append(DoD)\\n        i = i + 1\\n    UpI = pd.Series(UpI)\\n    DoI = pd.Series(DoI)\\n    PosDI = pd.Series(UpI.ewm(span=n, min_periods=n).mean())\\n    NegDI = pd.Series(DoI.ewm(span=n, min_periods=n).mean())\\n    RSI = pd.Series(round(PosDI * 100.0 / (PosDI + NegDI)), name=\\\"RSI_\\\" + str(n))\\n    return RSI\\n\\n\\ndef add_rsi(df: pd.DataFrame, column: str = \\\"close\\\", window: int = 14) -> pd.DataFrame:\\n    out = df.reset_index()\\n    rsi = relative_strength_index(out, window)\\n    df[f\\\"rsi_{column}_{window}\\\"] = pd.Series(data=rsi.values, index=df.index)\\n    return df\\n\\n\\n#################\\n\\n\\ndef np_racorr(array: np.ndarray, window: int, lag: int) -> np.ndarray:\\n    \\\"\\\"\\\"\\n    rolling autocorrelation\\n    \\\"\\\"\\\"\\n    return npx.rolling_apply(\\n        lambda array, lag: sm.tsa.acf(array, nlags=lag, fft=True)[lag],\\n        window,\\n        array,\\n        lag=lag,\\n        n_jobs=10,\\n    )\\n\\n\\ndef add_rolling_autocorr(\\n    df: pd.DataFrame, column: str, window: int, lag: int\\n) -> pd.DataFrame:\\n    log_changes_array = np.log(df[column]).diff().values\\n    df[f\\\"racorr_{column}_{window}\\\"] = np_racorr(log_changes_array, window, lag)\\n    return df\\n\\n\\n#################\\n@nb.njit\\ndef custom_percentile(array: np.ndarray) -> float:\\n    if (array.shape[0] - 1) == 0:\\n        return np.nan\\n    return (array[:-1] > array[-1]).sum() / (array.shape[0] - 1)\\n\\n\\ndef add_custom_percentile(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"rank_{column}_{window}\\\"] = npx.rolling_apply(\\n        custom_percentile, window, df[column].values, n_jobs=5\\n    )\\n    return df\";\n",
       "                var nbb_formatted_code = \"#################\\n\\n\\ndef add_log_returns(df: pd.DataFrame, column: str) -> pd.DataFrame:\\n    df[f\\\"log_{column}_return\\\"] = np.log(df[column]).diff()\\n    return df\\n\\n\\n#################\\ndef internal_bar_strength(df: pd.DataFrame) -> float:\\n    return (df.close - df.low) / (df.high - df.low)\\n\\n\\ndef add_internal_bar_strength(df: pd.DataFrame) -> pd.DataFrame:\\n    df[\\\"ibs\\\"] = internal_bar_strength(df)\\n    return df\\n\\n\\n#################\\n# @nb.jit\\ndef aqr_momentum(array: np.ndarray) -> float:\\n    \\\"\\\"\\\"\\n    Input:  Price time series.\\n    Output: Annualized exponential regression slope,\\n            multiplied by the R2\\n    \\\"\\\"\\\"\\n    returns = np.diff(np.log(array))  # .diff()\\n    x = np.arange(len(returns))\\n    slope, _, rvalue, _, _ = stats.linregress(x, returns)\\n    return ((1 + slope) ** 252) * (rvalue**2)  # annualize slope and multiply by R^2\\n\\n\\n@nb.njit\\ndef aqr_momo_numba(array: np.ndarray) -> float:\\n    y = np.diff(np.log(array))\\n    x = np.arange(y.shape[0])\\n    A = np.column_stack((x, np.ones(x.shape[0])))\\n    model, resid = np.linalg.lstsq(A, y)[:2]\\n    r2 = 1 - resid / (y.size * y.var())\\n    return (((1 + model[0]) ** 252) * r2)[0]\\n\\n\\ndef add_aqr_momentum(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"aqr_momo_{column}_{window}\\\"] = npx.rolling_apply(\\n        aqr_momentum, window, df[column].values, n_jobs=10\\n    )\\n    return df\\n\\n\\ndef add_aqr_momentum_numba(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"aqr_momo_{column}_{window}\\\"] = npx.rolling_apply(\\n        aqr_momo_numba, window, df[column].values, n_jobs=10\\n    )\\n    return df\\n\\n\\n#################\\n\\n\\ndef get_slope(array: np.ndarray) -> float:\\n    returns = np.diff(np.log(array))\\n    x = np.arange(len(returns))\\n    slope, _, rvalue, _, _ = stats.linregress(x, returns)\\n    return slope\\n\\n\\n@nb.njit\\ndef get_slope_numba(array: np.ndarray) -> float:\\n    y = np.diff(np.log(array))\\n    # y = y[~np.isnan(y)]\\n    x = np.arange(y.shape[0])\\n    A = np.column_stack((x, np.ones(x.shape[0])))\\n    model, resid = np.linalg.lstsq(A, y)[:2]\\n    return model[0]\\n\\n\\ndef add_slope_column(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"slope_{column}_{window}\\\"] = npx.rolling_apply(\\n        get_slope, window, df[column].values, n_jobs=10\\n    )\\n    return df\\n\\n\\ndef add_slope_column_numba(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"slope_{column}_{window}\\\"] = npx.rolling_apply(\\n        get_slope_numba, window, df[column].values, n_jobs=1\\n    )\\n    return df\\n\\n\\n#################\\ndef add_average_price(df: pd.DataFrame) -> pd.DataFrame:\\n    df[\\\"average_price\\\"] = (df.high + df.low + df.close + df.open) / 4\\n    return df\\n\\n\\n#################\\ndef add_rolling_min(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    array = npx.rolling_apply(np.min, window, df[column].values, n_jobs=10)\\n    df[f\\\"rmin_{column}_{window}\\\"] = array\\n    return df\\n\\n\\ndef add_rolling_max(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    array = npx.rolling_apply(np.max, window, df[column].values, n_jobs=10)\\n    df[f\\\"rmax_{column}_{window}\\\"] = array\\n    return df\\n\\n\\n#################\\n\\n# for some reason njit is generating zerodivision errors whereas numpy is not\\n@nb.njit\\ndef numba_vwap(\\n    avg: np.ndarray, v: np.ndarray, idx: np.ndarray, len_df: int, window: int\\n) -> np.ndarray:\\n    n = np.shape(np.arange(len_df - window))[0]\\n    A = np.empty((n, 2))\\n    for i in np.arange(len_df - window):\\n        tmp_avg = avg[i : i + window]\\n        tmp_v = v[i : i + window]\\n        aa = np.sum(tmp_v * tmp_avg) / np.sum(tmp_v)\\n        jj = idx[i + window]\\n        A[i, 0] = jj\\n        A[i, 1] = aa\\n    return A\\n\\n\\ndef numpy_vwap(\\n    avg: np.ndarray, v: np.ndarray, idx: np.ndarray, len_df: int, window: int\\n) -> np.ndarray:\\n    n = np.shape(np.arange(len_df - window))[0]\\n    A = np.empty((n, 2))\\n    for i in tqdm(np.arange(len_df - window)):\\n        tmp_avg = avg[i : i + window]\\n        tmp_v = v[i : i + window]\\n        aa = np.sum(tmp_v * tmp_avg) / np.sum(tmp_v)\\n        jj = idx[i + window]\\n        A[i, 0] = jj\\n        A[i, 1] = aa\\n    return A\\n\\n\\ndef add_rolling_vwap(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    v = df.volume.values\\n    avg = df[column].values\\n    idx = df.index.asi8\\n    # A = numba_vwap(avg, v, idx, len(df), window)\\n    A = numpy_vwap(avg, v, idx, len(df), window)\\n    outdf = (\\n        pd.DataFrame(A, columns=[\\\"index\\\", f\\\"rvwap_{window}\\\"])\\n        .assign(datetime=lambda df: pd.to_datetime(df[\\\"index\\\"], unit=\\\"ns\\\"))\\n        .drop(\\\"index\\\", axis=1)\\n        .set_index(\\\"datetime\\\")\\n    )\\n    df = df.join(outdf, how=\\\"left\\\")\\n    return df\\n\\n\\n#################\\ndef add_rolling_bands(\\n    df: pd.DataFrame, column: str, dist: int, window: int\\n) -> pd.DataFrame:\\n    upper = df[column] + dist * df[column].rolling(window).std()\\n    lower = df[column] - dist * df[column].rolling(window).std()\\n\\n    df[f\\\"upper_band_{column}\\\"] = upper\\n    df[f\\\"lower_band_{column}\\\"] = lower\\n    return df\\n\\n\\n#################\\ndef add_acceleration(\\n    df: pd.DataFrame, column: str = \\\"close\\\", window: int = 10\\n) -> pd.DataFrame:\\n    return_diff = df[column].pct_change().diff()\\n    df[f\\\"racc_{column}_{window}\\\"] = return_diff.rolling(\\n        window\\n    ).std()  # standard deviation of second deriv aka acceleration\\n    return df\\n\\n\\ndef roll_rank_bk(array):\\n    rank = array.size + 1 - bk.rankdata(array)[-1]\\n    A = array.shape[0]\\n    p = rank / A\\n    return p\\n\\n\\n#################\\ndef add_volatility(\\n    df: pd.DataFrame, column: str = \\\"close\\\", window: int = 10\\n) -> pd.DataFrame:\\n    returns = df[column].pct_change()\\n    df[f\\\"rvol_{column}_{window}\\\"] = returns.rolling(window).std()\\n    return df\\n\\n\\ndef relative_strength_index(df: pd.DataFrame, n: int) -> pd.Series:\\n    \\\"\\\"\\\"\\n    Calculate Relative Strength Index(RSI) for given data.\\n    https://github.com/Crypto-toolbox/pandas-technical-indicators/blob/master/technical_indicators.py\\n\\n    :param df: pandas.DataFrame\\n    :param n:\\n    :return: pandas.DataFrame\\n    \\\"\\\"\\\"\\n    i = 0\\n    UpI = [0]\\n    DoI = [0]\\n    while i + 1 <= df.index[-1]:\\n        UpMove = df.loc[i + 1, \\\"high\\\"] - df.loc[i, \\\"high\\\"]\\n        DoMove = df.loc[i, \\\"low\\\"] - df.loc[i + 1, \\\"low\\\"]\\n        if UpMove > DoMove and UpMove > 0:\\n            UpD = UpMove\\n        else:\\n            UpD = 0\\n        UpI.append(UpD)\\n        if DoMove > UpMove and DoMove > 0:\\n            DoD = DoMove\\n        else:\\n            DoD = 0\\n        DoI.append(DoD)\\n        i = i + 1\\n    UpI = pd.Series(UpI)\\n    DoI = pd.Series(DoI)\\n    PosDI = pd.Series(UpI.ewm(span=n, min_periods=n).mean())\\n    NegDI = pd.Series(DoI.ewm(span=n, min_periods=n).mean())\\n    RSI = pd.Series(round(PosDI * 100.0 / (PosDI + NegDI)), name=\\\"RSI_\\\" + str(n))\\n    return RSI\\n\\n\\ndef add_rsi(df: pd.DataFrame, column: str = \\\"close\\\", window: int = 14) -> pd.DataFrame:\\n    out = df.reset_index()\\n    rsi = relative_strength_index(out, window)\\n    df[f\\\"rsi_{column}_{window}\\\"] = pd.Series(data=rsi.values, index=df.index)\\n    return df\\n\\n\\n#################\\n\\n\\ndef np_racorr(array: np.ndarray, window: int, lag: int) -> np.ndarray:\\n    \\\"\\\"\\\"\\n    rolling autocorrelation\\n    \\\"\\\"\\\"\\n    return npx.rolling_apply(\\n        lambda array, lag: sm.tsa.acf(array, nlags=lag, fft=True)[lag],\\n        window,\\n        array,\\n        lag=lag,\\n        n_jobs=10,\\n    )\\n\\n\\ndef add_rolling_autocorr(\\n    df: pd.DataFrame, column: str, window: int, lag: int\\n) -> pd.DataFrame:\\n    log_changes_array = np.log(df[column]).diff().values\\n    df[f\\\"racorr_{column}_{window}\\\"] = np_racorr(log_changes_array, window, lag)\\n    return df\\n\\n\\n#################\\n@nb.njit\\ndef custom_percentile(array: np.ndarray) -> float:\\n    if (array.shape[0] - 1) == 0:\\n        return np.nan\\n    return (array[:-1] > array[-1]).sum() / (array.shape[0] - 1)\\n\\n\\ndef add_custom_percentile(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\\n    df[f\\\"rank_{column}_{window}\\\"] = npx.rolling_apply(\\n        custom_percentile, window, df[column].values, n_jobs=5\\n    )\\n    return df\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################\n",
    "\n",
    "\n",
    "def add_log_returns(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    df[f\"log_{column}_return\"] = np.log(df[column]).diff()\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "def internal_bar_strength(df: pd.DataFrame) -> float:\n",
    "    return (df.close - df.low) / (df.high - df.low)\n",
    "\n",
    "\n",
    "def add_internal_bar_strength(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"ibs\"] = internal_bar_strength(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "# @nb.jit\n",
    "def aqr_momentum(array: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Input:  Price time series.\n",
    "    Output: Annualized exponential regression slope, \n",
    "            multiplied by the R2\n",
    "    \"\"\"    \n",
    "    returns = np.diff(np.log(array))  # .diff()\n",
    "    x = np.arange(len(returns))\n",
    "    slope, _, rvalue, _, _ = stats.linregress(x, returns)\n",
    "    return ((1 + slope) ** 252) * (rvalue ** 2)  # annualize slope and multiply by R^2\n",
    "\n",
    "\n",
    "@nb.njit\n",
    "def aqr_momo_numba(array: np.ndarray) -> float:\n",
    "    y = np.diff(np.log(array))\n",
    "    x = np.arange(y.shape[0])\n",
    "    A = np.column_stack((x, np.ones(x.shape[0])))\n",
    "    model, resid = np.linalg.lstsq(A, y)[:2]\n",
    "    r2 = 1 - resid / (y.size * y.var())\n",
    "    return (((1 + model[0]) ** 252) * r2)[0]\n",
    "\n",
    "\n",
    "def add_aqr_momentum(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    df[f\"aqr_momo_{column}_{window}\"] = npx.rolling_apply(\n",
    "        aqr_momentum, window, df[column].values, n_jobs=10\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_aqr_momentum_numba(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    df[f\"aqr_momo_{column}_{window}\"] = npx.rolling_apply(\n",
    "        aqr_momo_numba, window, df[column].values, n_jobs=10\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "\n",
    "def get_slope(array: np.ndarray) -> float:\n",
    "    returns = np.diff(np.log(array))\n",
    "    x = np.arange(len(returns))\n",
    "    slope, _, rvalue, _, _ = stats.linregress(x, returns)\n",
    "    return slope\n",
    "\n",
    "\n",
    "@nb.njit\n",
    "def get_slope_numba(array: np.ndarray) -> float:\n",
    "    y = np.diff(np.log(array))\n",
    "    # y = y[~np.isnan(y)]\n",
    "    x = np.arange(y.shape[0])\n",
    "    A = np.column_stack((x, np.ones(x.shape[0])))\n",
    "    model, resid = np.linalg.lstsq(A, y)[:2]\n",
    "    return model[0]\n",
    "\n",
    "\n",
    "def add_slope_column(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    df[f\"slope_{column}_{window}\"] = npx.rolling_apply(\n",
    "        get_slope, window, df[column].values, n_jobs=10\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_slope_column_numba(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    df[f\"slope_{column}_{window}\"] = npx.rolling_apply(\n",
    "        get_slope_numba, window, df[column].values, n_jobs=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "def add_average_price(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"average_price\"] = (df.high + df.low + df.close + df.open) / 4\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "def add_rolling_min(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    array = npx.rolling_apply(np.min, window, df[column].values, n_jobs=10)\n",
    "    df[f\"rmin_{column}_{window}\"] = array\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_rolling_max(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    array = npx.rolling_apply(np.max, window, df[column].values, n_jobs=10)\n",
    "    df[f\"rmax_{column}_{window}\"] = array\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "# for some reason njit is generating zerodivision errors whereas numpy is not\n",
    "@nb.njit\n",
    "def numba_vwap(\n",
    "    avg: np.ndarray, v: np.ndarray, idx: np.ndarray, len_df: int, window: int\n",
    ") -> np.ndarray:\n",
    "    n = np.shape(np.arange(len_df - window))[0]\n",
    "    A = np.empty((n, 2))\n",
    "    for i in np.arange(len_df - window):\n",
    "        tmp_avg = avg[i : i + window]\n",
    "        tmp_v = v[i : i + window]\n",
    "        aa = np.sum(tmp_v * tmp_avg) / np.sum(tmp_v)\n",
    "        jj = idx[i + window]\n",
    "        A[i, 0] = jj\n",
    "        A[i, 1] = aa\n",
    "    return A\n",
    "\n",
    "\n",
    "def numpy_vwap(\n",
    "    avg: np.ndarray, v: np.ndarray, idx: np.ndarray, len_df: int, window: int\n",
    ") -> np.ndarray:\n",
    "    n = np.shape(np.arange(len_df - window))[0]\n",
    "    A = np.empty((n, 2))\n",
    "    for i in tqdm(np.arange(len_df - window)):\n",
    "        tmp_avg = avg[i : i + window]\n",
    "        tmp_v = v[i : i + window]\n",
    "        aa = np.sum(tmp_v * tmp_avg) / np.sum(tmp_v)\n",
    "        jj = idx[i + window]\n",
    "        A[i, 0] = jj\n",
    "        A[i, 1] = aa\n",
    "    return A\n",
    "\n",
    "\n",
    "def add_rolling_vwap(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    v = df.volume.values\n",
    "    avg = df[column].values\n",
    "    idx = df.index.asi8\n",
    "    # A = numba_vwap(avg, v, idx, len(df), window)\n",
    "    A = numpy_vwap(avg, v, idx, len(df), window)\n",
    "    outdf = (\n",
    "        pd.DataFrame(A, columns=[\"index\", f\"rvwap_{window}\"])\n",
    "        .assign(datetime=lambda df: pd.to_datetime(df[\"index\"], unit=\"ns\"))\n",
    "        .drop(\"index\", axis=1)\n",
    "        .set_index(\"datetime\")\n",
    "    )\n",
    "    df = df.join(outdf, how=\"left\")\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "def add_rolling_bands(\n",
    "    df: pd.DataFrame, column: str, dist: int, window: int\n",
    ") -> pd.DataFrame:\n",
    "    upper = df[column] + dist * df[column].rolling(window).std()\n",
    "    lower = df[column] - dist * df[column].rolling(window).std()\n",
    "\n",
    "    df[f\"upper_band_{column}\"] = upper\n",
    "    df[f\"lower_band_{column}\"] = lower\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "def add_acceleration(\n",
    "    df: pd.DataFrame, column: str = \"close\", window: int = 10\n",
    ") -> pd.DataFrame:\n",
    "    return_diff = df[column].pct_change().diff()\n",
    "    df[f\"racc_{column}_{window}\"] = return_diff.rolling(\n",
    "        window\n",
    "    ).std()  # standard deviation of second deriv aka acceleration\n",
    "    return df\n",
    "\n",
    "\n",
    "def roll_rank_bk(array):\n",
    "    rank = array.size + 1 - bk.rankdata(array)[-1]\n",
    "    A = array.shape[0]\n",
    "    p = rank / A\n",
    "    return p\n",
    "\n",
    "\n",
    "#################\n",
    "def add_volatility(\n",
    "    df: pd.DataFrame, column: str = \"close\", window: int = 10\n",
    ") -> pd.DataFrame:\n",
    "    returns = df[column].pct_change()\n",
    "    df[f\"rvol_{column}_{window}\"] = returns.rolling(window).std()\n",
    "    return df\n",
    "\n",
    "\n",
    "def relative_strength_index(df: pd.DataFrame, n: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate Relative Strength Index(RSI) for given data.\n",
    "    https://github.com/Crypto-toolbox/pandas-technical-indicators/blob/master/technical_indicators.py\n",
    "\n",
    "    :param df: pandas.DataFrame\n",
    "    :param n:\n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    UpI = [0]\n",
    "    DoI = [0]\n",
    "    while i + 1 <= df.index[-1]:\n",
    "        UpMove = df.loc[i + 1, \"high\"] - df.loc[i, \"high\"]\n",
    "        DoMove = df.loc[i, \"low\"] - df.loc[i + 1, \"low\"]\n",
    "        if UpMove > DoMove and UpMove > 0:\n",
    "            UpD = UpMove\n",
    "        else:\n",
    "            UpD = 0\n",
    "        UpI.append(UpD)\n",
    "        if DoMove > UpMove and DoMove > 0:\n",
    "            DoD = DoMove\n",
    "        else:\n",
    "            DoD = 0\n",
    "        DoI.append(DoD)\n",
    "        i = i + 1\n",
    "    UpI = pd.Series(UpI)\n",
    "    DoI = pd.Series(DoI)\n",
    "    PosDI = pd.Series(UpI.ewm(span=n, min_periods=n).mean())\n",
    "    NegDI = pd.Series(DoI.ewm(span=n, min_periods=n).mean())\n",
    "    RSI = pd.Series(round(PosDI * 100.0 / (PosDI + NegDI)), name=\"RSI_\" + str(n))\n",
    "    return RSI\n",
    "\n",
    "\n",
    "def add_rsi(df: pd.DataFrame, column: str = \"close\", window: int = 14) -> pd.DataFrame:\n",
    "    out = df.reset_index()\n",
    "    rsi = relative_strength_index(out, window)\n",
    "    df[f\"rsi_{column}_{window}\"] = pd.Series(data=rsi.values, index=df.index)\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "\n",
    "def np_racorr(array: np.ndarray, window: int, lag: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    rolling autocorrelation\n",
    "    \"\"\"\n",
    "    return npx.rolling_apply(\n",
    "        lambda array, lag: sm.tsa.acf(array, nlags=lag, fft=True)[lag],\n",
    "        window,\n",
    "        array,\n",
    "        lag=lag,\n",
    "        n_jobs=10,\n",
    "    )\n",
    "\n",
    "\n",
    "def add_rolling_autocorr(\n",
    "    df: pd.DataFrame, column: str, window: int, lag: int\n",
    ") -> pd.DataFrame:\n",
    "    log_changes_array = np.log(df[column]).diff().values\n",
    "    df[f\"racorr_{column}_{window}\"] = np_racorr(log_changes_array, window, lag)\n",
    "    return df\n",
    "\n",
    "\n",
    "#################\n",
    "@nb.njit\n",
    "def custom_percentile(array: np.ndarray) -> float:\n",
    "    if (array.shape[0] - 1) == 0:\n",
    "        return np.nan\n",
    "    return (array[:-1] > array[-1]).sum() / (array.shape[0] - 1)\n",
    "\n",
    "\n",
    "def add_custom_percentile(df: pd.DataFrame, column: str, window: int) -> pd.DataFrame:\n",
    "    df[f\"rank_{column}_{window}\"] = npx.rolling_apply(\n",
    "        custom_percentile, window, df[column].values, n_jobs=5\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data store for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T23:51:04.242285Z",
     "start_time": "2021-03-29T23:51:03.749602Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-01T13:47:37.118925Z",
     "iopub.status.busy": "2022-12-01T13:47:37.118925Z",
     "iopub.status.idle": "2022-12-01T13:47:37.366202Z",
     "shell.execute_reply": "2022-12-01T13:47:37.366202Z",
     "shell.execute_reply.started": "2022-12-01T13:47:37.118925Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"HDF_FILEPATH = processed / 'spy_features.h5'\\n\\nstore = pd.HDFStore(\\n    HDF_FILEPATH, mode=\\\"a\\\", complevel=1, complib=\\\"blosc:lz4\\\"\\n)\\nstore.close()\";\n",
       "                var nbb_formatted_code = \"HDF_FILEPATH = processed / \\\"spy_features.h5\\\"\\n\\nstore = pd.HDFStore(HDF_FILEPATH, mode=\\\"a\\\", complevel=1, complib=\\\"blosc:lz4\\\")\\nstore.close()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HDF_FILEPATH = processed / 'spy_features.h5'\n",
    "\n",
    "store = pd.HDFStore(\n",
    "    HDF_FILEPATH, mode=\"a\", complevel=1, complib=\"blosc:lz4\"\n",
    ")\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T23:51:04.537852Z",
     "start_time": "2021-03-29T23:51:04.246275Z"
    }
   },
   "source": [
    "### add and save features function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T14:15:13.771521Z",
     "start_time": "2021-05-26T14:15:13.548306Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-01T13:48:06.747613Z",
     "iopub.status.busy": "2022-12-01T13:48:06.746616Z",
     "iopub.status.idle": "2022-12-01T13:48:06.912173Z",
     "shell.execute_reply": "2022-12-01T13:48:06.912173Z",
     "shell.execute_reply.started": "2022-12-01T13:48:06.746616Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"def add_and_save_features(data, data_frequency, periods, hdf_filepath, rank_window=10):\\n    \\\"\\\"\\\"\\n    function to create features according to frequency and periods and save to\\n    hdf store. store must already be created.\\n\\n    # Args\\n        data: dict, keys are frequency, values are dataframes\\n        data_frequency: str, one of the data frequencies from the data dict\\n        periods: dict, keys are period labels, values are the integers\\n        hdf_filepath: pathlib or str object\\n        rank_window: int to divide window by for ranking features\\n\\n    \\\"\\\"\\\"\\n\\n    log_errors = []\\n    pprint(periods)\\n\\n    df = data[data_frequency].copy()\\n\\n    for key, window in tqdm(periods.items()):\\n        tqdm._instances.clear()\\n        try:\\n            tmp_df = (\\n                df.pipe(add_average_price)\\n                .pipe(add_rolling_vwap, column=\\\"average_price\\\", window=window)\\n                .pipe(\\n                    add_rolling_bands, column=f\\\"rvwap_{window}\\\", dist=2, window=window\\n                )\\n                .pipe(add_internal_bar_strength)\\n                .pipe(add_rolling_min, column=\\\"low\\\", window=window)\\n                .pipe(add_rolling_max, column=\\\"high\\\", window=window)\\n                .dropna()\\n                .pipe(\\n                    add_slope_column_numba,\\n                    column=f\\\"lower_band_rvwap_{window}\\\",\\n                    window=window,\\n                )\\n                .pipe(\\n                    add_slope_column_numba,\\n                    column=f\\\"upper_band_rvwap_{window}\\\",\\n                    window=window,\\n                )\\n                .pipe(\\n                    add_slope_column_numba, column=f\\\"rmin_low_{window}\\\", window=window\\n                )\\n                .pipe(\\n                    add_slope_column_numba, column=f\\\"rmax_high_{window}\\\", window=window\\n                )\\n                .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n                .pipe(add_aqr_momentum_numba, column=\\\"close\\\", window=window)\\n                .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n                .pipe(add_aqr_momentum_numba, column=\\\"average_price\\\", window=window)\\n                .pipe(add_acceleration, column=f\\\"rvwap_{window}\\\", window=window)\\n                .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n                .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n                .pipe(add_aqr_momentum_numba, column=f\\\"rvwap_{window}\\\", window=window)\\n                .pipe(add_volatility, column=\\\"close\\\", window=window)\\n                .pipe(add_volatility, column=\\\"average_price\\\", window=window)\\n                .pipe(add_rsi, column=\\\"close\\\", window=window)\\n                .pipe(add_rsi, column=\\\"average_price\\\", window=window)\\n                .pipe(add_rolling_autocorr, column=\\\"close\\\", window=window, lag=1)\\n                .pipe(\\n                    add_rolling_autocorr, column=\\\"average_price\\\", window=window, lag=1\\n                )\\n            )\\n            columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\\\"up\\\") :]\\n            for column in tqdm(columns_to_rank):\\n                tmp_df = add_custom_percentile(\\n                    tmp_df, column, window=max(int(window // rank_window), rank_window)\\n                )\\n                # ^^ make window smaller since this basically a double lag\\n\\n            # write to store iteratively in case of problems\\n            with pd.HDFStore(hdf_filepath) as store:\\n                store.put(\\n                    value=tmp_df, key=f\\\"spy/{data_frequency}/{key}\\\", format=\\\"table\\\"\\n                )\\n\\n        except Exception as error:\\n            log_error = dict(window=window, error=error)\\n            log_errors.append(log_error)\\n    pprint(f\\\"{data_frequency} errors:\\\\n{log_errors}\\\")\\n    return\";\n",
       "                var nbb_formatted_code = \"def add_and_save_features(data, data_frequency, periods, hdf_filepath, rank_window=10):\\n    \\\"\\\"\\\"\\n    function to create features according to frequency and periods and save to\\n    hdf store. store must already be created.\\n\\n    # Args\\n        data: dict, keys are frequency, values are dataframes\\n        data_frequency: str, one of the data frequencies from the data dict\\n        periods: dict, keys are period labels, values are the integers\\n        hdf_filepath: pathlib or str object\\n        rank_window: int to divide window by for ranking features\\n\\n    \\\"\\\"\\\"\\n\\n    log_errors = []\\n    pprint(periods)\\n\\n    df = data[data_frequency].copy()\\n\\n    for key, window in tqdm(periods.items()):\\n        tqdm._instances.clear()\\n        try:\\n            tmp_df = (\\n                df.pipe(add_average_price)\\n                .pipe(add_rolling_vwap, column=\\\"average_price\\\", window=window)\\n                .pipe(\\n                    add_rolling_bands, column=f\\\"rvwap_{window}\\\", dist=2, window=window\\n                )\\n                .pipe(add_internal_bar_strength)\\n                .pipe(add_rolling_min, column=\\\"low\\\", window=window)\\n                .pipe(add_rolling_max, column=\\\"high\\\", window=window)\\n                .dropna()\\n                .pipe(\\n                    add_slope_column_numba,\\n                    column=f\\\"lower_band_rvwap_{window}\\\",\\n                    window=window,\\n                )\\n                .pipe(\\n                    add_slope_column_numba,\\n                    column=f\\\"upper_band_rvwap_{window}\\\",\\n                    window=window,\\n                )\\n                .pipe(\\n                    add_slope_column_numba, column=f\\\"rmin_low_{window}\\\", window=window\\n                )\\n                .pipe(\\n                    add_slope_column_numba, column=f\\\"rmax_high_{window}\\\", window=window\\n                )\\n                .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n                .pipe(add_aqr_momentum_numba, column=\\\"close\\\", window=window)\\n                .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n                .pipe(add_aqr_momentum_numba, column=\\\"average_price\\\", window=window)\\n                .pipe(add_acceleration, column=f\\\"rvwap_{window}\\\", window=window)\\n                .pipe(add_acceleration, column=\\\"close\\\", window=window)\\n                .pipe(add_acceleration, column=\\\"average_price\\\", window=window)\\n                .pipe(add_aqr_momentum_numba, column=f\\\"rvwap_{window}\\\", window=window)\\n                .pipe(add_volatility, column=\\\"close\\\", window=window)\\n                .pipe(add_volatility, column=\\\"average_price\\\", window=window)\\n                .pipe(add_rsi, column=\\\"close\\\", window=window)\\n                .pipe(add_rsi, column=\\\"average_price\\\", window=window)\\n                .pipe(add_rolling_autocorr, column=\\\"close\\\", window=window, lag=1)\\n                .pipe(\\n                    add_rolling_autocorr, column=\\\"average_price\\\", window=window, lag=1\\n                )\\n            )\\n            columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\\\"up\\\") :]\\n            for column in tqdm(columns_to_rank):\\n                tmp_df = add_custom_percentile(\\n                    tmp_df, column, window=max(int(window // rank_window), rank_window)\\n                )\\n                # ^^ make window smaller since this basically a double lag\\n\\n            # write to store iteratively in case of problems\\n            with pd.HDFStore(hdf_filepath) as store:\\n                store.put(\\n                    value=tmp_df, key=f\\\"spy/{data_frequency}/{key}\\\", format=\\\"table\\\"\\n                )\\n\\n        except Exception as error:\\n            log_error = dict(window=window, error=error)\\n            log_errors.append(log_error)\\n    pprint(f\\\"{data_frequency} errors:\\\\n{log_errors}\\\")\\n    return\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_and_save_features(data, data_frequency, periods, hdf_filepath, rank_window=10):\n",
    "    \"\"\"\n",
    "    function to create features according to frequency and periods and save to\n",
    "    hdf store. store must already be created.\n",
    "\n",
    "    # Args\n",
    "        data: dict, keys are frequency, values are dataframes\n",
    "        data_frequency: str, one of the data frequencies from the data dict\n",
    "        periods: dict, keys are period labels, values are the integers\n",
    "        hdf_filepath: pathlib or str object\n",
    "        rank_window: int to divide window by for ranking features\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    log_errors = []\n",
    "    pprint(periods)\n",
    "\n",
    "    df = data[data_frequency].copy()\n",
    "\n",
    "    for key, window in tqdm(periods.items()):\n",
    "        tqdm._instances.clear()\n",
    "        try:\n",
    "            tmp_df = (\n",
    "                df.pipe(add_average_price)\n",
    "                .pipe(add_rolling_vwap, column=\"average_price\", window=window)\n",
    "                .pipe(\n",
    "                    add_rolling_bands, column=f\"rvwap_{window}\", dist=2, window=window\n",
    "                )\n",
    "                .pipe(add_internal_bar_strength)\n",
    "                .pipe(add_rolling_min, column=\"low\", window=window)\n",
    "                .pipe(add_rolling_max, column=\"high\", window=window)\n",
    "                .dropna()\n",
    "                .pipe(\n",
    "                    add_slope_column_numba,\n",
    "                    column=f\"lower_band_rvwap_{window}\",\n",
    "                    window=window,\n",
    "                )\n",
    "                .pipe(\n",
    "                    add_slope_column_numba,\n",
    "                    column=f\"upper_band_rvwap_{window}\",\n",
    "                    window=window,\n",
    "                )\n",
    "                .pipe(\n",
    "                    add_slope_column_numba, column=f\"rmin_low_{window}\", window=window\n",
    "                )\n",
    "                .pipe(\n",
    "                    add_slope_column_numba, column=f\"rmax_high_{window}\", window=window\n",
    "                )\n",
    "                .pipe(add_acceleration, column=\"close\", window=window)\n",
    "                .pipe(add_aqr_momentum_numba, column=\"close\", window=window)\n",
    "                .pipe(add_acceleration, column=\"average_price\", window=window)\n",
    "                .pipe(add_aqr_momentum_numba, column=\"average_price\", window=window)\n",
    "                .pipe(add_acceleration, column=f\"rvwap_{window}\", window=window)\n",
    "                .pipe(add_acceleration, column=\"close\", window=window)\n",
    "                .pipe(add_acceleration, column=\"average_price\", window=window)\n",
    "                .pipe(add_aqr_momentum_numba, column=f\"rvwap_{window}\", window=window)\n",
    "                .pipe(add_volatility, column=\"close\", window=window)\n",
    "                .pipe(add_volatility, column=\"average_price\", window=window)\n",
    "                .pipe(add_rsi, column=\"close\", window=window)\n",
    "                .pipe(add_rsi, column=\"average_price\", window=window)\n",
    "                .pipe(add_rolling_autocorr, column=\"close\", window=window, lag=1)\n",
    "                .pipe(\n",
    "                    add_rolling_autocorr, column=\"average_price\", window=window, lag=1\n",
    "                )\n",
    "            )\n",
    "            columns_to_rank = tmp_df.columns[tmp_df.columns.get_loc(\"up\") :]\n",
    "            for column in tqdm(columns_to_rank):\n",
    "                tmp_df = add_custom_percentile(\n",
    "                    tmp_df, column, window=max(int(window // rank_window), rank_window)\n",
    "                )\n",
    "                # ^^ make window smaller since this basically a double lag\n",
    "\n",
    "            # write to store iteratively in case of problems\n",
    "            with pd.HDFStore(hdf_filepath) as store:\n",
    "                store.put(\n",
    "                    value=tmp_df, key=f\"spy/{data_frequency}/{key}\", format=\"table\"\n",
    "                )\n",
    "\n",
    "        except Exception as error:\n",
    "            log_error = dict(window=window, error=error)\n",
    "            log_errors.append(log_error)\n",
    "    pprint(f\"{data_frequency} errors:\\n{log_errors}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 min multi day features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T13:37:09.105474Z",
     "start_time": "2021-05-26T13:37:07.038144Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-01T13:48:13.598997Z",
     "iopub.status.busy": "2022-12-01T13:48:13.598997Z",
     "iopub.status.idle": "2022-12-01T14:24:48.834026Z",
     "shell.execute_reply": "2022-12-01T14:24:48.833074Z",
     "shell.execute_reply.started": "2022-12-01T13:48:13.598997Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10_day': 2880,\n",
      " '1_day': 288,\n",
      " '21_day': 6048,\n",
      " '2_day': 576,\n",
      " '3_day': 864,\n",
      " '5_day': 1440}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 297285/297285 [00:03<00:00, 81264.96it/s]\n",
      "100%|| 26/26 [03:11<00:00,  7.38s/it]\n",
      "100%|| 296997/296997 [00:03<00:00, 75463.29it/s]\n",
      "100%|| 26/26 [03:04<00:00,  7.10s/it]\n",
      "100%|| 296709/296709 [00:04<00:00, 72195.80it/s]\n",
      "100%|| 26/26 [03:22<00:00,  7.77s/it]\n",
      "100%|| 296133/296133 [00:04<00:00, 63538.53it/s]\n",
      "100%|| 26/26 [03:16<00:00,  7.55s/it]\n",
      "100%|| 294693/294693 [00:05<00:00, 51948.26it/s]\n",
      "100%|| 26/26 [03:05<00:00,  7.14s/it]\n",
      "100%|| 291525/291525 [00:07<00:00, 41146.69it/s]\n",
      "100%|| 26/26 [03:10<00:00,  7.32s/it]\n",
      "100%|| 6/6 [36:35<00:00, 365.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'5m errors:\\n[]'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"one_day_in_minutes = 1440\\none_day = int(one_day_in_minutes // 5)\\ntwo_days = int(one_day * 2)\\nthree_days = int(one_day * 3)\\nfive_days = int(one_day * 5)\\nten_days = int(one_day * 10)\\none_month = int(one_day * 21)\\n\\nperiod_labels = [\\\"1_day\\\", \\\"2_day\\\", \\\"3_day\\\", \\\"5_day\\\", \\\"10_day\\\", \\\"21_day\\\"]\\nperiods = dict(\\n    zip(period_labels, [one_day, two_days, three_days, five_days, ten_days, one_month])\\n)\\n\\nFREQ = '5m'\\nadd_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=10)\";\n",
       "                var nbb_formatted_code = \"one_day_in_minutes = 1440\\none_day = int(one_day_in_minutes // 5)\\ntwo_days = int(one_day * 2)\\nthree_days = int(one_day * 3)\\nfive_days = int(one_day * 5)\\nten_days = int(one_day * 10)\\none_month = int(one_day * 21)\\n\\nperiod_labels = [\\\"1_day\\\", \\\"2_day\\\", \\\"3_day\\\", \\\"5_day\\\", \\\"10_day\\\", \\\"21_day\\\"]\\nperiods = dict(\\n    zip(period_labels, [one_day, two_days, three_days, five_days, ten_days, one_month])\\n)\\n\\nFREQ = \\\"5m\\\"\\nadd_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=10)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "one_day_in_minutes = 1440\n",
    "one_day = int(one_day_in_minutes // 5)\n",
    "two_days = int(one_day * 2)\n",
    "three_days = int(one_day * 3)\n",
    "five_days = int(one_day * 5)\n",
    "ten_days = int(one_day * 10)\n",
    "one_month = int(one_day * 21)\n",
    "\n",
    "period_labels = [\"1_day\", \"2_day\", \"3_day\", \"5_day\", \"10_day\", \"21_day\"]\n",
    "periods = dict(\n",
    "    zip(period_labels, [one_day, two_days, three_days, five_days, ten_days, one_month])\n",
    ")\n",
    "\n",
    "FREQ = '5m'\n",
    "add_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 min multi day features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T00:42:32.781498Z",
     "start_time": "2021-03-30T00:42:32.553109Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-01T15:45:58.814028Z",
     "iopub.status.busy": "2022-12-01T15:45:58.814028Z",
     "iopub.status.idle": "2022-12-01T15:53:07.028885Z",
     "shell.execute_reply": "2022-12-01T15:53:07.027888Z",
     "shell.execute_reply.started": "2022-12-01T15:45:58.814028Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10_day': 480,\n",
      " '1_day': 48,\n",
      " '21_day': 1008,\n",
      " '2_day': 96,\n",
      " '3_day': 144,\n",
      " '5_day': 240}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 52706/52706 [00:00<00:00, 87544.08it/s]\n",
      "100%|| 26/26 [00:38<00:00,  1.48s/it]\n",
      "100%|| 52658/52658 [00:00<00:00, 86983.31it/s]\n",
      "100%|| 26/26 [00:39<00:00,  1.51s/it]\n",
      "100%|| 52610/52610 [00:00<00:00, 79925.48it/s]\n",
      "100%|| 26/26 [00:41<00:00,  1.59s/it]\n",
      "100%|| 52514/52514 [00:00<00:00, 77392.61it/s]\n",
      "100%|| 26/26 [00:40<00:00,  1.55s/it]\n",
      "100%|| 52274/52274 [00:00<00:00, 78936.58it/s]\n",
      "100%|| 26/26 [00:40<00:00,  1.56s/it]\n",
      "100%|| 51746/51746 [00:00<00:00, 70658.91it/s]\n",
      "100%|| 26/26 [00:40<00:00,  1.55s/it]\n",
      "100%|| 6/6 [07:08<00:00, 71.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'30m errors:\\n[]'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"one_day_in_minutes = 1440\\none_day = int(one_day_in_minutes // 30)\\ntwo_days = int(one_day * 2)\\nthree_days = int(one_day * 3)\\nfive_days = int(one_day * 5)\\nten_days = int(one_day * 10)\\none_month = int(one_day * 21)\\n\\nperiod_labels = [\\\"1_day\\\", \\\"2_day\\\", \\\"3_day\\\", \\\"5_day\\\", \\\"10_day\\\", \\\"21_day\\\"]\\nperiods = dict(\\n    zip(period_labels, [one_day, two_days, three_days, five_days, ten_days, one_month])\\n)\\n\\nFREQ = \\\"30m\\\"\\nadd_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=10)\";\n",
       "                var nbb_formatted_code = \"one_day_in_minutes = 1440\\none_day = int(one_day_in_minutes // 30)\\ntwo_days = int(one_day * 2)\\nthree_days = int(one_day * 3)\\nfive_days = int(one_day * 5)\\nten_days = int(one_day * 10)\\none_month = int(one_day * 21)\\n\\nperiod_labels = [\\\"1_day\\\", \\\"2_day\\\", \\\"3_day\\\", \\\"5_day\\\", \\\"10_day\\\", \\\"21_day\\\"]\\nperiods = dict(\\n    zip(period_labels, [one_day, two_days, three_days, five_days, ten_days, one_month])\\n)\\n\\nFREQ = \\\"30m\\\"\\nadd_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=10)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "one_day_in_minutes = 1440\n",
    "one_day = int(one_day_in_minutes // 30)\n",
    "two_days = int(one_day * 2)\n",
    "three_days = int(one_day * 3)\n",
    "five_days = int(one_day * 5)\n",
    "ten_days = int(one_day * 10)\n",
    "one_month = int(one_day * 21)\n",
    "\n",
    "period_labels = [\"1_day\", \"2_day\", \"3_day\", \"5_day\", \"10_day\", \"21_day\"]\n",
    "periods = dict(\n",
    "    zip(period_labels, [one_day, two_days, three_days, five_days, ten_days, one_month])\n",
    ")\n",
    "\n",
    "FREQ = \"30m\"\n",
    "add_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Hour (60 min) multi day features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T00:59:36.628731Z",
     "start_time": "2021-03-30T00:52:17.331466Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-01T15:53:07.029882Z",
     "iopub.status.busy": "2022-12-01T15:53:07.029882Z",
     "iopub.status.idle": "2022-12-01T15:59:03.726629Z",
     "shell.execute_reply": "2022-12-01T15:59:03.726629Z",
     "shell.execute_reply.started": "2022-12-01T15:53:07.029882Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10_day': 240,\n",
      " '1_day': 24,\n",
      " '21_day': 504,\n",
      " '2_day': 48,\n",
      " '3_day': 72,\n",
      " '5_day': 120,\n",
      " '63_day': 1512}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 30125/30125 [00:00<00:00, 79273.41it/s]\n",
      "100%|| 26/26 [00:27<00:00,  1.06s/it]\n",
      "100%|| 30101/30101 [00:00<00:00, 83144.60it/s]\n",
      "100%|| 26/26 [00:27<00:00,  1.04s/it]\n",
      "100%|| 30077/30077 [00:00<00:00, 81068.66it/s]\n",
      "100%|| 26/26 [00:27<00:00,  1.06s/it]\n",
      "100%|| 30029/30029 [00:00<00:00, 85781.80it/s]\n",
      "100%|| 26/26 [00:27<00:00,  1.07s/it]\n",
      "100%|| 29909/29909 [00:00<00:00, 76784.12it/s]\n",
      "100%|| 26/26 [00:27<00:00,  1.04s/it]\n",
      "100%|| 29645/29645 [00:00<00:00, 76412.29it/s]\n",
      "100%|| 26/26 [00:27<00:00,  1.05s/it]\n",
      "100%|| 28637/28637 [00:00<00:00, 66308.32it/s]\n",
      "100%|| 26/26 [00:26<00:00,  1.02s/it]\n",
      "100%|| 7/7 [05:56<00:00, 50.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1H errors:\\n[]'\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"one_day_in_minutes = 1440\\none_day = int(one_day_in_minutes // 60)\\ntwo_days = int(one_day * 2)\\nthree_days = int(one_day * 3)\\nfive_days = int(one_day * 5)\\nten_days = int(one_day * 10)\\none_month = int(one_day * 21)\\nthree_month = int(one_month * 3)\\n\\nperiod_labels = [\\\"1_day\\\", \\\"2_day\\\", \\\"3_day\\\", \\\"5_day\\\", \\\"10_day\\\", \\\"21_day\\\", \\\"63_day\\\"]\\nperiods = dict(\\n    zip(\\n        period_labels,\\n        [one_day, two_days, three_days, five_days, ten_days, one_month, three_month],\\n    )\\n)\\n\\nFREQ = \\\"1H\\\"\\nadd_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=10)\";\n",
       "                var nbb_formatted_code = \"one_day_in_minutes = 1440\\none_day = int(one_day_in_minutes // 60)\\ntwo_days = int(one_day * 2)\\nthree_days = int(one_day * 3)\\nfive_days = int(one_day * 5)\\nten_days = int(one_day * 10)\\none_month = int(one_day * 21)\\nthree_month = int(one_month * 3)\\n\\nperiod_labels = [\\\"1_day\\\", \\\"2_day\\\", \\\"3_day\\\", \\\"5_day\\\", \\\"10_day\\\", \\\"21_day\\\", \\\"63_day\\\"]\\nperiods = dict(\\n    zip(\\n        period_labels,\\n        [one_day, two_days, three_days, five_days, ten_days, one_month, three_month],\\n    )\\n)\\n\\nFREQ = \\\"1H\\\"\\nadd_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=10)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "one_day_in_minutes = 1440\n",
    "one_day = int(one_day_in_minutes // 60)\n",
    "two_days = int(one_day * 2)\n",
    "three_days = int(one_day * 3)\n",
    "five_days = int(one_day * 5)\n",
    "ten_days = int(one_day * 10)\n",
    "one_month = int(one_day * 21)\n",
    "three_month = int(one_month * 3)\n",
    "\n",
    "period_labels = [\"1_day\", \"2_day\", \"3_day\", \"5_day\", \"10_day\", \"21_day\", \"63_day\"]\n",
    "periods = dict(\n",
    "    zip(\n",
    "        period_labels,\n",
    "        [one_day, two_days, three_days, five_days, ten_days, one_month, three_month],\n",
    "    )\n",
    ")\n",
    "\n",
    "FREQ = \"1H\"\n",
    "add_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## daily features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T01:03:19.895463Z",
     "start_time": "2021-03-30T00:59:36.630727Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-01T15:59:03.727598Z",
     "iopub.status.busy": "2022-12-01T15:59:03.727598Z",
     "iopub.status.idle": "2022-12-01T16:02:35.097846Z",
     "shell.execute_reply": "2022-12-01T16:02:35.097846Z",
     "shell.execute_reply.started": "2022-12-01T15:59:03.727598Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10_day': 10,\n",
      " '126_day': 126,\n",
      " '21_day': 21,\n",
      " '252_day': 252,\n",
      " '2_day': 2,\n",
      " '3_day': 3,\n",
      " '5_day': 5,\n",
      " '63_day': 63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3772/3772 [00:00<00:00, 85956.60it/s]\n",
      "100%|| 26/26 [00:11<00:00,  2.26it/s]\n",
      "100%|| 3771/3771 [00:00<00:00, 75622.00it/s]\n",
      "100%|| 26/26 [00:11<00:00,  2.18it/s]\n",
      "100%|| 3769/3769 [00:00<00:00, 75581.17it/s]\n",
      "100%|| 26/26 [00:11<00:00,  2.22it/s]\n",
      "100%|| 3764/3764 [00:00<00:00, 80304.38it/s]\n",
      "100%|| 26/26 [00:11<00:00,  2.26it/s]\n",
      "100%|| 3753/3753 [00:00<00:00, 76794.69it/s]\n",
      "100%|| 26/26 [00:13<00:00,  1.99it/s]\n",
      "100%|| 3711/3711 [00:00<00:00, 67654.77it/s]\n",
      "100%|| 26/26 [00:11<00:00,  2.28it/s]\n",
      "100%|| 3648/3648 [00:00<00:00, 77826.77it/s]\n",
      "100%|| 26/26 [00:11<00:00,  2.29it/s]\n",
      "100%|| 3522/3522 [00:00<00:00, 72071.44it/s]\n",
      "100%|| 26/26 [00:11<00:00,  2.31it/s]\n",
      "100%|| 8/8 [03:31<00:00, 26.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1D errors:\\n[]'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"one_day = 1\\ntwo_days = int(one_day * 2)\\nthree_days = int(one_day * 3)\\nfive_days = int(one_day * 5)\\nten_days = int(one_day * 10)\\none_month = int(one_day * 21)\\nthree_month = int(one_month * 3)\\nsix_month = int(one_month * 6)\\ntwelve_month = int(one_month * 12)\\n\\nperiod_labels = [  # \\\"1_day\\\",\\n    \\\"2_day\\\",\\n    \\\"3_day\\\",\\n    \\\"5_day\\\",\\n    \\\"10_day\\\",\\n    \\\"21_day\\\",\\n    \\\"63_day\\\",\\n    \\\"126_day\\\",\\n    \\\"252_day\\\",\\n]\\nperiods = dict(\\n    zip(\\n        period_labels,\\n        [  # one_day,\\n            two_days,\\n            three_days,\\n            five_days,\\n            ten_days,\\n            one_month,\\n            three_month,\\n            six_month,\\n            twelve_month,\\n        ],\\n    )\\n)\\n\\nFREQ = \\\"1D\\\"\\nadd_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=2)\";\n",
       "                var nbb_formatted_code = \"one_day = 1\\ntwo_days = int(one_day * 2)\\nthree_days = int(one_day * 3)\\nfive_days = int(one_day * 5)\\nten_days = int(one_day * 10)\\none_month = int(one_day * 21)\\nthree_month = int(one_month * 3)\\nsix_month = int(one_month * 6)\\ntwelve_month = int(one_month * 12)\\n\\nperiod_labels = [  # \\\"1_day\\\",\\n    \\\"2_day\\\",\\n    \\\"3_day\\\",\\n    \\\"5_day\\\",\\n    \\\"10_day\\\",\\n    \\\"21_day\\\",\\n    \\\"63_day\\\",\\n    \\\"126_day\\\",\\n    \\\"252_day\\\",\\n]\\nperiods = dict(\\n    zip(\\n        period_labels,\\n        [  # one_day,\\n            two_days,\\n            three_days,\\n            five_days,\\n            ten_days,\\n            one_month,\\n            three_month,\\n            six_month,\\n            twelve_month,\\n        ],\\n    )\\n)\\n\\nFREQ = \\\"1D\\\"\\nadd_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "one_day = 1\n",
    "two_days = int(one_day * 2)\n",
    "three_days = int(one_day * 3)\n",
    "five_days = int(one_day * 5)\n",
    "ten_days = int(one_day * 10)\n",
    "one_month = int(one_day * 21)\n",
    "three_month = int(one_month * 3)\n",
    "six_month = int(one_month * 6)\n",
    "twelve_month = int(one_month * 12)\n",
    "\n",
    "period_labels = [  # \"1_day\",\n",
    "    \"2_day\",\n",
    "    \"3_day\",\n",
    "    \"5_day\",\n",
    "    \"10_day\",\n",
    "    \"21_day\",\n",
    "    \"63_day\",\n",
    "    \"126_day\",\n",
    "    \"252_day\",\n",
    "]\n",
    "periods = dict(\n",
    "    zip(\n",
    "        period_labels,\n",
    "        [  # one_day,\n",
    "            two_days,\n",
    "            three_days,\n",
    "            five_days,\n",
    "            ten_days,\n",
    "            one_month,\n",
    "            three_month,\n",
    "            six_month,\n",
    "            twelve_month,\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "FREQ = \"1D\"\n",
    "add_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weekly features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T01:05:39.432425Z",
     "start_time": "2021-03-30T01:03:19.897408Z"
    },
    "execution": {
     "iopub.execute_input": "2022-12-01T16:02:35.099842Z",
     "iopub.status.busy": "2022-12-01T16:02:35.099842Z",
     "iopub.status.idle": "2022-12-01T16:04:54.077601Z",
     "shell.execute_reply": "2022-12-01T16:04:54.077601Z",
     "shell.execute_reply.started": "2022-12-01T16:02:35.099842Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'12_month': 48,\n",
      " '1_month': 4,\n",
      " '2_week': 2,\n",
      " '3_month': 12,\n",
      " '3_week': 3,\n",
      " '6_month': 24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 781/781 [00:00<00:00, 71188.77it/s]\n",
      "100%|| 26/26 [00:09<00:00,  2.60it/s]\n",
      "100%|| 780/780 [00:00<00:00, 65180.85it/s]\n",
      "100%|| 26/26 [00:10<00:00,  2.60it/s]\n",
      "100%|| 779/779 [00:00<00:00, 65097.28it/s]\n",
      "100%|| 26/26 [00:09<00:00,  2.68it/s]\n",
      "100%|| 771/771 [00:00<00:00, 70284.90it/s]\n",
      "100%|| 26/26 [00:09<00:00,  2.75it/s]\n",
      "100%|| 759/759 [00:00<00:00, 69189.47it/s]\n",
      "100%|| 26/26 [00:11<00:00,  2.34it/s]\n",
      "100%|| 735/735 [00:00<00:00, 73698.62it/s]\n",
      "100%|| 26/26 [00:07<00:00,  3.36it/s]\n",
      "100%|| 6/6 [02:18<00:00, 23.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1W errors:\\n[]'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"one_week = 1\\ntwo_weeks = int(one_week * 2)\\nthree_weeks = int(one_week * 3)\\none_month = int(one_week * 4)\\nthree_month = int(one_month * 3)\\nsix_month = int(one_month * 6)\\ntwelve_month = int(one_month * 12)\\n\\nperiod_labels = [\\n    # \\\"1_week\\\",\\n    \\\"2_week\\\",\\n    \\\"3_week\\\",\\n    \\\"1_month\\\",\\n    \\\"3_month\\\",\\n    \\\"6_month\\\",\\n    \\\"12_month\\\",\\n]\\nperiods = dict(\\n    zip(\\n        period_labels,\\n        [\\n            # one_week,\\n            two_weeks,\\n            three_weeks,\\n            one_month,\\n            three_month,\\n            six_month,\\n            twelve_month,\\n        ],\\n    )\\n)\\n\\nFREQ = \\\"1W\\\"\\nadd_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=2)\";\n",
       "                var nbb_formatted_code = \"one_week = 1\\ntwo_weeks = int(one_week * 2)\\nthree_weeks = int(one_week * 3)\\none_month = int(one_week * 4)\\nthree_month = int(one_month * 3)\\nsix_month = int(one_month * 6)\\ntwelve_month = int(one_month * 12)\\n\\nperiod_labels = [\\n    # \\\"1_week\\\",\\n    \\\"2_week\\\",\\n    \\\"3_week\\\",\\n    \\\"1_month\\\",\\n    \\\"3_month\\\",\\n    \\\"6_month\\\",\\n    \\\"12_month\\\",\\n]\\nperiods = dict(\\n    zip(\\n        period_labels,\\n        [\\n            # one_week,\\n            two_weeks,\\n            three_weeks,\\n            one_month,\\n            three_month,\\n            six_month,\\n            twelve_month,\\n        ],\\n    )\\n)\\n\\nFREQ = \\\"1W\\\"\\nadd_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "one_week = 1\n",
    "two_weeks = int(one_week * 2)\n",
    "three_weeks = int(one_week * 3)\n",
    "one_month = int(one_week * 4)\n",
    "three_month = int(one_month * 3)\n",
    "six_month = int(one_month * 6)\n",
    "twelve_month = int(one_month * 12)\n",
    "\n",
    "period_labels = [\n",
    "    # \"1_week\",\n",
    "    \"2_week\",\n",
    "    \"3_week\",\n",
    "    \"1_month\",\n",
    "    \"3_month\",\n",
    "    \"6_month\",\n",
    "    \"12_month\",\n",
    "]\n",
    "periods = dict(\n",
    "    zip(\n",
    "        period_labels,\n",
    "        [\n",
    "            # one_week,\n",
    "            two_weeks,\n",
    "            three_weeks,\n",
    "            one_month,\n",
    "            three_month,\n",
    "            six_month,\n",
    "            twelve_month,\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "FREQ = \"1W\"\n",
    "add_and_save_features(data, FREQ, periods, HDF_FILEPATH, rank_window=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
